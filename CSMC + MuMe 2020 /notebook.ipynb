{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OrchDataset import RawDatabase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instr_filter = ['Hn', 'Ob', 'Vn', 'Va',\n",
    "                    'Vc', 'Fl', 'Tbn', 'Bn', 'TpC', 'ClBb']\n",
    "\n",
    "rdb = RawDatabase('./TinySOL', 10, instr_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "for instr, lst in rdb.db.items():\n",
    "    for item in lst:\n",
    "        i += 1\n",
    "#         for y in item:\n",
    "            \n",
    "i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Loaded 15 target samples\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for OrchMatchNet:\n\tsize mismatch for net.linear2.weight: copying a param with shape torch.Size([424, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for net.linear2.bias: copying a param with shape torch.Size([424]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-28dba5265623>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-28dba5265623>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, state_path, data, targets)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for OrchMatchNet:\n\tsize mismatch for net.linear2.weight: copying a param with shape torch.Size([424, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for net.linear2.bias: copying a param with shape torch.Size([424]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import random\n",
    "import soundfile as sf\n",
    "\n",
    "from model import OrchMatchNet\n",
    "from OrchDataset import RawDatabase\n",
    "from train import getPosNMax\n",
    "from parameters import GLOBAL_PARAMS, SimParams\n",
    "\n",
    "\"\"\" \n",
    "To run this file:\n",
    "- You need a folder of targets to be orchestrated, and set 'target_path' to point to this\n",
    "- You need to create a folder to store the orchestrated solutions, and set 'solutions_path'\n",
    "to be the path to this folder\n",
    "- You need a trained model saved as a .pth file, and set 'state_path' to point to this\n",
    "\n",
    "- Set 'tinysol_path' to point to your TinySOL database\n",
    "- Set model_type, instr_filter, and n\n",
    "\n",
    "\"\"\"\n",
    "# The argument must be the folder where the params.pkl file is\n",
    "GLOBAL_PARAMS.load_parameters('./orchestrated_targets/params_resnet')\n",
    "\n",
    "# path to TinySOL data\n",
    "tinysol_path = './TinySOL'\n",
    "\n",
    "# cnn or resnet\n",
    "model_type = 'resnet'\n",
    "\n",
    "# instruments to be used (all instruments will be used)\n",
    "instr_filter = ['Hn', 'Ob', 'Vn', 'Va',\n",
    "                'Vc', 'Fl', 'Tbn', 'Bn', 'TpC', 'ClBb']\n",
    "\n",
    "# path to target samples\n",
    "target_path = './target_samples'\n",
    "\n",
    "# path to store solutions as .wav\n",
    "solutions_path = './orchestrated_targets/{}_n={}'.format(model_type, len(instr_filter))\n",
    "\n",
    "# path to a trained version of the model\n",
    "# state_path = './orchestrated_targets/params_resnet/epoch_24.pth'\n",
    "state_path = 'resnet_n=10.pth'\n",
    "\n",
    "# number of samples to be used in solution\n",
    "n = 10\n",
    "\n",
    "# if sanity_check, then targets are TinySOL combinations instead of real targets to be orchestrated\n",
    "sanity_check = False\n",
    "\n",
    "\n",
    "def test(model, state_path, data, targets):\n",
    "    device = torch.device('cpu')\n",
    "    state = torch.load(state_path, map_location=device)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    outputs = model(data)\n",
    "    outputs = outputs.detach().cpu().clone().numpy()\n",
    "\n",
    "    if sanity_check:\n",
    "        accuracy = evaluate(outputs, labels)\n",
    "        print(accuracy)\n",
    "\n",
    "    # text file to write solutions to\n",
    "    f = open(solutions_path + '/orchestration_results.txt', 'w+')\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        output = outputs[i]\n",
    "        target = targets[i]\n",
    "        # get indices of top n probabilities\n",
    "        indices = getPosNMax(output, n)\n",
    "        # turn indices into [instr, pitch]\n",
    "        classes = get_classes(indices)\n",
    "        # get top n probabiltiies\n",
    "        probs = [output[i] for i in indices]\n",
    "        # turn probabilities into dynamic markings (pp, mf, ff)\n",
    "        dynamics = prob_to_dynamic(probs)\n",
    "        # combine into (instr, pitch, dynamic)\n",
    "        for j in range(len(classes)):\n",
    "            classes[j].append(dynamics[j])\n",
    "        \n",
    "        # turn (instr, pitch, dynamic) into list of actual TinySOL sample paths\n",
    "        sample_paths = find_sample_paths(classes)\n",
    "        target['classes'] = classes\n",
    "\n",
    "        # combine samples\n",
    "        mixed_file, sr = combine(sample_paths, target)\n",
    "        # write wav\n",
    "        file_name = solutions_path + '/orchestrated_' + target['name'] + '.wav'\n",
    "        sf.write(file_name, mixed_file, sr)\n",
    "\n",
    "        if sanity_check:\n",
    "            target['distance'] = 0\n",
    "        else:\n",
    "            target['distance'] = compute_distance(target, mixed_file)\n",
    "        \n",
    "        # write to text file\n",
    "        f.write('Target: {}; Distance: {:,.2f}\\nSamples used: {}\\n'.format(target['name'], target['distance'], target['classes']))\n",
    "\n",
    "    # compute avg distance\n",
    "    sum = 0\n",
    "    for target in targets:\n",
    "        sum += target['distance']\n",
    "    sum /= len(targets)\n",
    "    f.write('Average distance: {:,.2f}'.format(sum))\n",
    "    f.close()\n",
    "\n",
    "        \n",
    "def find_sample_paths(classes):\n",
    "    '''\n",
    "    classes is a list where each element is a list like this: [instr, pitch, dynamic]\n",
    "    '''\n",
    "    samples = []\n",
    "    for c in classes:\n",
    "        instr, pitch, dynamic = c\n",
    "        instr_samples = rdb.db[instr]\n",
    "        for lst in instr_samples:\n",
    "            for sample in lst:\n",
    "                if sample['instrument'] == instr and sample['pitch_name'] == pitch and sample['nuance'] == dynamic:\n",
    "                    samples.append(sample['path'])\n",
    "                    break\n",
    "    return samples\n",
    "\n",
    "'''\n",
    "converts a list of probabilities to a list of dynamics\n",
    "0 - 0.33 -> pp\n",
    "0.34 - 0.66 -> mf\n",
    "0.67 - 1 -> ff\n",
    "'''\n",
    "def prob_to_dynamic(probs):\n",
    "    dynamics = []\n",
    "    pp = 0.33\n",
    "    mf = 0.66\n",
    "    for prob in probs:\n",
    "        if prob > mf:\n",
    "            dynamics.append('ff')\n",
    "        elif prob > pp:\n",
    "            dynamics.append('mf')\n",
    "        else:\n",
    "            dynamics.append('pp')\n",
    "    return dynamics    \n",
    "\n",
    "# given a list of indices, return the corresponding [instrument, pitch] in lab_class\n",
    "def get_classes(indices):\n",
    "    classes = [None for i in indices]\n",
    "    for instrument, pitch_dict in lab_class.items():\n",
    "        for pitch, class_number in pitch_dict.items():\n",
    "            if class_number in indices:\n",
    "                index = indices.index(class_number)\n",
    "                classes[index] = [instrument, pitch]\n",
    "    assert len(indices) == len(classes)\n",
    "    return classes\n",
    "\n",
    "# load a sample at the given path and return the melspectrogram  and duration of the sample\n",
    "def load_sample(path):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    mel_hop_length = sr * duration / (87 - 1) # based on training data size\n",
    "    mel_hop_length = int(mel_hop_length)\n",
    "   \n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, hop_length=mel_hop_length)\n",
    "    return torch.tensor(np.array([mel_spec])), duration\n",
    "\n",
    "\n",
    "# load in a folder of data and return a list of melspectrograms\n",
    "def load_data(folder_path):\n",
    "    data = []\n",
    "    targets = []\n",
    "    for entry in os.listdir(folder_path):\n",
    "        if entry.endswith('.wav'):\n",
    "            full_path = os.path.join(folder_path, entry)\n",
    "            mel_spec, duration = load_sample(full_path)\n",
    "            data.append(mel_spec)\n",
    "            targets.append({'name': entry[:-4], 'duration': duration, 'path': full_path})\n",
    "    print(\"Loaded {} target samples\".format(len(data)))\n",
    "\n",
    "    return torch.stack(data), targets\n",
    "\n",
    "\n",
    "def combine(soundlist, target):\n",
    "    mixed_file = np.zeros((1, 1))\n",
    "    for sound in soundlist:\n",
    "        sfile, sr = librosa.load(sound, sr=None)\n",
    "        mixed_file = mix(mixed_file, sfile)\n",
    "    mixed_file = mixed_file/len(soundlist)\n",
    "\n",
    "    # trim to target length\n",
    "    trim_index = int(target['duration']*sr)\n",
    "    mixed_file = mixed_file[:trim_index]\n",
    "\n",
    "    return mixed_file, sr\n",
    "\n",
    "\n",
    "def compute_distance(target, solution):\n",
    "    target, _ = librosa.load(target['path'], sr=None)\n",
    "\n",
    "    # if the target is longer than the solution, must trim the target\n",
    "    if (target.size > solution.size):\n",
    "        target = target[:solution.size]\n",
    "\n",
    "    target_fft = np.fft.fft(target)\n",
    "    solution_fft = np.fft.fft(solution)\n",
    "\n",
    "    lambda_1 = 1\n",
    "    lambda_2 = 1\n",
    "\n",
    "    sum_1 = 0\n",
    "    sum_2 = 0\n",
    "    for i in range(target_fft.size):\n",
    "        a = target_fft[i]\n",
    "        b = solution_fft[i]\n",
    "        if a - b < 0:\n",
    "            sum_1 += a - b\n",
    "            sum_2 += abs(a - b)\n",
    "    distance = lambda_1 * sum_1 + lambda_2 * sum_2\n",
    "    return float(distance)\n",
    "\n",
    "\n",
    "def mix(fa, fb):\n",
    "    diff = len(fa) - len(fb)\n",
    "\n",
    "    if diff >= 0:\n",
    "        add = np.zeros((1, diff), dtype=np.float32)\n",
    "        fb = np.append(fb, add)\n",
    "    else:\n",
    "        add = np.zeros((1, -diff), dtype=np.float32)\n",
    "        fa = np.append(fa, add)\n",
    "\n",
    "    return fa+fb\n",
    "\n",
    "\n",
    "def make_fake_targets(num_classes):\n",
    "    num_targets = 10\n",
    "    data = []\n",
    "    targets = []\n",
    "    labels = []\n",
    "    for i in range(num_targets):\n",
    "        samples = random.sample(range(num_classes), n)\n",
    "        labels.append(samples)\n",
    "        samples = get_classes(samples)\n",
    "        dynamics = [random.choice(['pp', 'mf', 'ff']) for _ in range(n)]\n",
    "        for j in range(len(samples)):\n",
    "            samples[j].append(dynamics[j])\n",
    "        paths = find_sample_paths(samples)\n",
    "        mixed_file, _ = combine(paths, {'duration':4})\n",
    "        mel_spec = librosa.feature.melspectrogram(y=mixed_file,sr=44100,hop_length=GLOBAL_PARAMS.MEL_HOP_LENGTH)[:128,:87]\n",
    "        data.append(torch.tensor([mel_spec]))\n",
    "        \n",
    "        name = ''\n",
    "        for l in samples:\n",
    "            name += l[0] + l[1] + '_'\n",
    "        target = {'name':name[:-1], 'duration':4}\n",
    "        targets.append(target)\n",
    "\n",
    "    encoded_labels = []\n",
    "    for label in labels:\n",
    "        l = np.zeros(num_classes, dtype=np.float32)\n",
    "        for x in label:\n",
    "            l[x] = 1.0\n",
    "        encoded_labels.append(l)\n",
    "    encoded_labels = np.array(encoded_labels)\n",
    "\n",
    "    print('Created {} fake data targets'.format(num_targets))\n",
    "\n",
    "    return torch.stack(data), targets, encoded_labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     rdb = RawDatabase(tinysol_path, GLOBAL_PARAMS.rdm_granularity, instr_filter)\n",
    "\n",
    " \n",
    "    lab_class = GLOBAL_PARAMS.lab_class\n",
    "    num_classes = len(lab_class)\n",
    "    \n",
    "    def class_encoder(list_samp):\n",
    "        label = [0 for i in range(tot_size)]\n",
    "        for s in list_samp:\n",
    "            label[lab_class[s['instrument']][s['pitch_name']]] = 1\n",
    "        return np.array(label).astype(np.float32)\n",
    "\n",
    "    def evaluate(preds, labels):\n",
    "        if preds.shape != labels.shape:\n",
    "            print(\"[Error]: size difference\")\n",
    "        # compute the label-based accuracy\n",
    "        result = {}\n",
    "\n",
    "        result['acc'] = np.sum(preds*labels)/max(1.0, np.sum(labels))\n",
    "        pitch_acc = {}\n",
    "        for i in lab_class:\n",
    "            l = [lab_class[i][x] for x in lab_class[i]]\n",
    "            f = np.zeros(preds.shape, dtype=np.float32)\n",
    "            f[:, min(l):max(l)+1] = 1.0\n",
    "            f = labels*f\n",
    "            pitch_acc[i] = np.sum(preds*f)/max(1.0, np.sum(f))\n",
    "        result['pitch_acc'] = pitch_acc\n",
    "\n",
    "        return result\n",
    "    \n",
    "    features_shape = [128, 87] # the dim of the data used to train the network\n",
    "    \n",
    "    model = OrchMatchNet(num_classes, model_type, features_shape)\n",
    "    \n",
    "    \n",
    "    if sanity_check:\n",
    "        data, targets, labels = make_fake_targets(num_classes)\n",
    "    else:\n",
    "        data, targets = load_data(target_path)\n",
    "\n",
    "    test(model, state_path, data, targets)\n",
    "    print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
